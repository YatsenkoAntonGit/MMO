{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lr6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrEgRfDQecpn",
        "outputId": "0e342078-1443-4558-ea4d-e1e88fabe433"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, Tuple\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from nltk import WordPunctTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%matplotlib inline \n",
        "sns.set(style=\"ticks\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdP13CXZedaz"
      },
      "source": [
        "categories = [\"alt.atheism\", \"comp.sys.mac.hardware\", \"rec.autos\", \"sci.space\"]\n",
        "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
        "data = newsgroups['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_yFbFjWeddL"
      },
      "source": [
        "def accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Вычисление метрики accuracy для каждого класса\n",
        "    y_true - истинные значения классов\n",
        "    y_pred - предсказанные значения классов\n",
        "    Возвращает словарь: ключ - метка класса, \n",
        "    значение - Accuracy для данного класса\n",
        "    \"\"\"\n",
        "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
        "    d = {'t': y_true, 'p': y_pred}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    # Метки классов\n",
        "    classes = np.unique(y_true)\n",
        "    # Результирующий словарь\n",
        "    res = dict()\n",
        "    # Перебор меток классов\n",
        "    for c in classes:\n",
        "        # отфильтруем данные, которые соответствуют \n",
        "        # текущей метке класса в истинных значениях\n",
        "        temp_data_flt = df[df['t']==c]\n",
        "        # расчет accuracy для заданной метки класса\n",
        "        temp_acc = accuracy_score(\n",
        "            temp_data_flt['t'].values, \n",
        "            temp_data_flt['p'].values)\n",
        "        # сохранение результата в словарь\n",
        "        res[c] = temp_acc\n",
        "    return res\n",
        "\n",
        "def print_accuracy_score_for_classes(\n",
        "    y_true: np.ndarray, \n",
        "    y_pred: np.ndarray):\n",
        "    \"\"\"\n",
        "    Вывод метрики accuracy для каждого класса\n",
        "    \"\"\"\n",
        "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
        "    if len(accs)>0:\n",
        "        print('Метка \\t Accuracy')\n",
        "    for i in accs:\n",
        "        print('{} \\t {}'.format(i, accs[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmNUA8v0edg-",
        "outputId": "2a4bc531-0d78-4cc8-9a8a-6628ccc98bae"
      },
      "source": [
        "# CountVectorizer\n",
        "vocabVect = CountVectorizer()\n",
        "vocabVect.fit(data)\n",
        "corpusVocab = vocabVect.vocabulary_\n",
        "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество сформированных признаков - 32952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfzUfbbLedjX",
        "outputId": "13f6cf42-3ab6-44e5-b9a1-4b3b67e4bc61"
      },
      "source": [
        "for i in list(corpusVocab)[1:10]:\n",
        "    print('{}={}'.format(i, corpusVocab[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "higgins=15932\n",
            "fnalf=14077\n",
            "fnal=14075\n",
            "gov=15103\n",
            "bill=7052\n",
            "beam=6745\n",
            "jockey=17874\n",
            "subject=28761\n",
            "re=25040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U15wMcoGedlw",
        "outputId": "2c37ca9a-a2a7-4807-ffa5-c0ceaa956cf7"
      },
      "source": [
        "test_features = vocabVect.transform(data)\n",
        "test_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<2245x32952 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 333292 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30J_QbkwedoK",
        "outputId": "1753bd70-6a57-42b7-950f-bb08f09be456"
      },
      "source": [
        "# Размер нулевой строки\n",
        "len(test_features.todense()[0].getA1())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32952"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrDL955Hedqt",
        "outputId": "b5bd3d8b-0f84-43ae-f0d8-016c51495582"
      },
      "source": [
        "# Непустые значения нулевой строки\n",
        "[i for i in test_features.todense()[0].getA1() if i>0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 11,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZdNBDyNAzUz",
        "outputId": "c8df2319-9877-4063-8b2e-68567da59437"
      },
      "source": [
        "vocabVect.get_feature_names()[100:120]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['024150',\n",
              " '024246',\n",
              " '024423',\n",
              " '0245',\n",
              " '024626',\n",
              " '025',\n",
              " '025924',\n",
              " '0273',\n",
              " '0283',\n",
              " '03',\n",
              " '030',\n",
              " '0300',\n",
              " '030031',\n",
              " '0300ff',\n",
              " '030334',\n",
              " '030734',\n",
              " '031',\n",
              " '031905saundrsg',\n",
              " '0320',\n",
              " '0324']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chQPCh_xAzYo"
      },
      "source": [
        "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
        "    for v in vectorizers_list:\n",
        "        for c in classifiers_list:\n",
        "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
        "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
        "            print('Векторизация - {}'.format(v))\n",
        "            print('Модель для классификации - {}'.format(c))\n",
        "            print('Accuracy = {}'.format(score))\n",
        "            print('===========================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea6StqpQAzaP",
        "outputId": "e3814ad2-ac6e-4002-f03a-71a85dbb5e99"
      },
      "source": [
        "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
        "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
        "VectorizeAndClassify(vectorizers_list, classifiers_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None,\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
            "                            '000406': 7, '00041032': 8, '0004136': 9,\n",
            "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
            "                            '0004847546': 13, '0005': 14, '000601': 15,\n",
            "                            '000710': 16, '0009': 17, '00090711': 18,\n",
            "                            '000mi': 19, '000miles': 20, '001125': 21,\n",
            "                            '001127': 22, '0012': 23, '001428': 24,\n",
            "                            '001555': 25, '001718': 26, '001757': 27,\n",
            "                            '0018': 28, '0020': 29, ...})\n",
            "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Accuracy = 0.9576791634240783\n",
            "===========================\n",
            "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None,\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
            "                            '000406': 7, '00041032': 8, '0004136': 9,\n",
            "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
            "                            '0004847546': 13, '0005': 14, '000601': 15,\n",
            "                            '000710': 16, '0009': 17, '00090711': 18,\n",
            "                            '000mi': 19, '000miles': 20, '001125': 21,\n",
            "                            '001127': 22, '0012': 23, '001428': 24,\n",
            "                            '001555': 25, '001718': 26, '001757': 27,\n",
            "                            '0018': 28, '0020': 29, ...})\n",
            "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "Accuracy = 0.960351770274805\n",
            "===========================\n",
            "Векторизация - CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None,\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
            "                            '000406': 7, '00041032': 8, '0004136': 9,\n",
            "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
            "                            '0004847546': 13, '0005': 14, '000601': 15,\n",
            "                            '000710': 16, '0009': 17, '00090711': 18,\n",
            "                            '000mi': 19, '000miles': 20, '001125': 21,\n",
            "                            '001127': 22, '0012': 23, '001428': 24,\n",
            "                            '001555': 25, '001718': 26, '001757': 27,\n",
            "                            '0018': 28, '0020': 29, ...})\n",
            "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='uniform')\n",
            "Accuracy = 0.6788344054699196\n",
            "===========================\n",
            "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use...\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
            "                            '000406': 7, '00041032': 8, '0004136': 9,\n",
            "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
            "                            '0004847546': 13, '0005': 14, '000601': 15,\n",
            "                            '000710': 16, '0009': 17, '00090711': 18,\n",
            "                            '000mi': 19, '000miles': 20, '001125': 21,\n",
            "                            '001127': 22, '0012': 23, '001428': 24,\n",
            "                            '001555': 25, '001718': 26, '001757': 27,\n",
            "                            '0018': 28, '0020': 29, ...})\n",
            "Модель для классификации - LogisticRegression(C=3.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Accuracy = 0.9692596664834158\n",
            "===========================\n",
            "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use...\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
            "                            '000406': 7, '00041032': 8, '0004136': 9,\n",
            "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
            "                            '0004847546': 13, '0005': 14, '000601': 15,\n",
            "                            '000710': 16, '0009': 17, '00090711': 18,\n",
            "                            '000mi': 19, '000miles': 20, '001125': 21,\n",
            "                            '001127': 22, '0012': 23, '001428': 24,\n",
            "                            '001555': 25, '001718': 26, '001757': 27,\n",
            "                            '0018': 28, '0020': 29, ...})\n",
            "Модель для классификации - LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "Accuracy = 0.9812875872524032\n",
            "===========================\n",
            "Векторизация - TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use...\n",
            "                vocabulary={'00': 0, '000': 1, '0000': 2, '00000': 3,\n",
            "                            '000000': 4, '000021': 5, '000062david42': 6,\n",
            "                            '000406': 7, '00041032': 8, '0004136': 9,\n",
            "                            '0004246': 10, '0004422': 11, '00044513': 12,\n",
            "                            '0004847546': 13, '0005': 14, '000601': 15,\n",
            "                            '000710': 16, '0009': 17, '00090711': 18,\n",
            "                            '000mi': 19, '000miles': 20, '001125': 21,\n",
            "                            '001127': 22, '0012': 23, '001428': 24,\n",
            "                            '001555': 25, '001718': 26, '001757': 27,\n",
            "                            '0018': 28, '0020': 29, ...})\n",
            "Модель для классификации - KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
            "                     weights='uniform')\n",
            "Accuracy = 0.8971099909802493\n",
            "===========================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A_gWi3dA_c7"
      },
      "source": [
        "# word2vec \n",
        "# Подготовим корпус\n",
        "corpus = []\n",
        "stop_words = stopwords.words('english')\n",
        "tok = WordPunctTokenizer()\n",
        "for line in newsgroups['data']:\n",
        "    line1 = line.strip().lower()\n",
        "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
        "    text_tok = tok.tokenize(line1)\n",
        "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
        "    corpus.append(text_tok1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSx3CLcmA_e2",
        "outputId": "8b3b072d-c84d-4968-f9ca-b02d6ac2426d"
      },
      "source": [
        "corpus[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['higgins',\n",
              "  'fnalf',\n",
              "  'fnal',\n",
              "  'gov',\n",
              "  'bill',\n",
              "  'higgins',\n",
              "  'beam',\n",
              "  'jockey',\n",
              "  'subject',\n",
              "  'get',\n",
              "  'comet',\n",
              "  'temporary',\n",
              "  'orbit',\n",
              "  'around',\n",
              "  'jupiter',\n",
              "  'organization',\n",
              "  'fermi',\n",
              "  'national',\n",
              "  'accelerator',\n",
              "  'laboratory',\n",
              "  'lines',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'fnalf',\n",
              "  'fnal',\n",
              "  'gov',\n",
              "  'article',\n",
              "  'apr',\n",
              "  'stortek',\n",
              "  'com',\n",
              "  'pg',\n",
              "  'sanitas',\n",
              "  'stortek',\n",
              "  'com',\n",
              "  'paul',\n",
              "  'gilmartin',\n",
              "  'writes',\n",
              "  'bill',\n",
              "  'higgins',\n",
              "  'beam',\n",
              "  'jockey',\n",
              "  'higgins',\n",
              "  'fnalf',\n",
              "  'fnal',\n",
              "  'gov',\n",
              "  'wrote',\n",
              "  'comet',\n",
              "  'experts',\n",
              "  'explain',\n",
              "  'comet',\n",
              "  'gets',\n",
              "  'jovian',\n",
              "  'orbit',\n",
              "  'begin',\n",
              "  'non',\n",
              "  'gravitational',\n",
              "  'forces',\n",
              "  'heating',\n",
              "  'outgassing',\n",
              "  'comet',\n",
              "  'gets',\n",
              "  'inner',\n",
              "  'solar',\n",
              "  'system',\n",
              "  'forget',\n",
              "  'galilean',\n",
              "  'satellites',\n",
              "  'jupiter',\n",
              "  'poor',\n",
              "  'old',\n",
              "  'physics',\n",
              "  'intuition',\n",
              "  'surprised',\n",
              "  'tiny',\n",
              "  'masses',\n",
              "  'sitting',\n",
              "  'close',\n",
              "  'jupiter',\n",
              "  'play',\n",
              "  'role',\n",
              "  'whatsoever',\n",
              "  'problem',\n",
              "  'put',\n",
              "  'technically',\n",
              "  'extra',\n",
              "  'volume',\n",
              "  'add',\n",
              "  'phase',\n",
              "  'space',\n",
              "  'possible',\n",
              "  'capture',\n",
              "  'trajectories',\n",
              "  'negligible',\n",
              "  'jupiter',\n",
              "  'e',\n",
              "  'kg',\n",
              "  'galilean',\n",
              "  'satellites',\n",
              "  'around',\n",
              "  'e',\n",
              "  'also',\n",
              "  'said',\n",
              "  'references',\n",
              "  'looked',\n",
              "  'mention',\n",
              "  'outgassing',\n",
              "  'breakup',\n",
              "  'important',\n",
              "  'processes',\n",
              "  'important',\n",
              "  'thing',\n",
              "  'jupiter',\n",
              "  'sun',\n",
              "  'comet',\n",
              "  'reverse',\n",
              "  'slingshot',\n",
              "  'leads',\n",
              "  'weakly',\n",
              "  'jupiter',\n",
              "  'bound',\n",
              "  'orbit',\n",
              "  'comet',\n",
              "  'least',\n",
              "  'temporary',\n",
              "  'one',\n",
              "  'bill',\n",
              "  'higgins',\n",
              "  'late',\n",
              "  'night',\n",
              "  'still',\n",
              "  'doth',\n",
              "  'haunt',\n",
              "  'fermilab',\n",
              "  'dressed',\n",
              "  'garments',\n",
              "  'soaked',\n",
              "  'brine',\n",
              "  'bitnet',\n",
              "  'higgins',\n",
              "  'fnal',\n",
              "  'bitnet',\n",
              "  'though',\n",
              "  'life',\n",
              "  'used',\n",
              "  'hug',\n",
              "  'internet',\n",
              "  'higgins',\n",
              "  'fnal',\n",
              "  'fnal',\n",
              "  'gov',\n",
              "  'dead',\n",
              "  'draw',\n",
              "  'line',\n",
              "  'span',\n",
              "  'hepnet',\n",
              "  'higgins',\n",
              "  'tragedy',\n",
              "  'clementine'],\n",
              " ['semmett',\n",
              "  'gmuvax',\n",
              "  'gmu',\n",
              "  'edu',\n",
              "  'steve',\n",
              "  'emmett',\n",
              "  'subject',\n",
              "  'moscow',\n",
              "  'aviation',\n",
              "  'institute',\n",
              "  'summer',\n",
              "  'school',\n",
              "  'organization',\n",
              "  'george',\n",
              "  'mason',\n",
              "  'university',\n",
              "  'fairfax',\n",
              "  'virginia',\n",
              "  'usa',\n",
              "  'lines',\n",
              "  'attached',\n",
              "  'copy',\n",
              "  'announcement',\n",
              "  'picked',\n",
              "  'trip',\n",
              "  'moscow',\n",
              "  'last',\n",
              "  'week',\n",
              "  'several',\n",
              "  'friends',\n",
              "  'moscow',\n",
              "  'aviation',\n",
              "  'institute',\n",
              "  'asked',\n",
              "  'post',\n",
              "  'announcement',\n",
              "  'done',\n",
              "  'editing',\n",
              "  'contents',\n",
              "  'unchanged',\n",
              "  'original',\n",
              "  'announcement',\n",
              "  'familiar',\n",
              "  'moscow',\n",
              "  'aviation',\n",
              "  'institute',\n",
              "  'leading',\n",
              "  'russian',\n",
              "  'school',\n",
              "  'higher',\n",
              "  'education',\n",
              "  'dedicated',\n",
              "  'training',\n",
              "  'aircraft',\n",
              "  'spacecraft',\n",
              "  'designers',\n",
              "  'specializes',\n",
              "  'airframe',\n",
              "  'design',\n",
              "  'powerplant',\n",
              "  'design',\n",
              "  'control',\n",
              "  'systems',\n",
              "  'power',\n",
              "  'systems',\n",
              "  'virtually',\n",
              "  'major',\n",
              "  'former',\n",
              "  'soviet',\n",
              "  'airframe',\n",
              "  'designers',\n",
              "  'tupolev',\n",
              "  'su',\n",
              "  'iluchine',\n",
              "  'migoyan',\n",
              "  'etc',\n",
              "  'schooled',\n",
              "  'mai',\n",
              "  'opportunity',\n",
              "  'tour',\n",
              "  'two',\n",
              "  'museums',\n",
              "  'maintained',\n",
              "  'mai',\n",
              "  'aircraft',\n",
              "  'include',\n",
              "  'mig',\n",
              "  'su',\n",
              "  'yak',\n",
              "  'cockpit',\n",
              "  'f',\n",
              "  'among',\n",
              "  'others',\n",
              "  'fascinating',\n",
              "  'eye',\n",
              "  'opening',\n",
              "  'experience',\n",
              "  'expecially',\n",
              "  'given',\n",
              "  'fact',\n",
              "  'museum',\n",
              "  'year',\n",
              "  'ago',\n",
              "  'closed',\n",
              "  'virtually',\n",
              "  'everyone',\n",
              "  'also',\n",
              "  'opportunity',\n",
              "  'see',\n",
              "  'experiments',\n",
              "  'conducted',\n",
              "  'plasma',\n",
              "  'drive',\n",
              "  'engines',\n",
              "  'future',\n",
              "  'space',\n",
              "  'craft',\n",
              "  'use',\n",
              "  'questions',\n",
              "  'institute',\n",
              "  'program',\n",
              "  'would',\n",
              "  'glad',\n",
              "  'try',\n",
              "  'answer',\n",
              "  'institute',\n",
              "  'faculty',\n",
              "  'e',\n",
              "  'mail',\n",
              "  'addresses',\n",
              "  'however',\n",
              "  'takes',\n",
              "  'day',\n",
              "  'receiver',\n",
              "  'get',\n",
              "  'message',\n",
              "  'still',\n",
              "  'bit',\n",
              "  'antiquated',\n",
              "  'rapidly',\n",
              "  'changing',\n",
              "  'steve',\n",
              "  'emmett',\n",
              "  'semmett',\n",
              "  'gmuvax',\n",
              "  'gmu',\n",
              "  'edu',\n",
              "  'ps',\n",
              "  'please',\n",
              "  'send',\n",
              "  'questions',\n",
              "  'via',\n",
              "  'e',\n",
              "  'mail',\n",
              "  'george',\n",
              "  'mason',\n",
              "  'university',\n",
              "  'week',\n",
              "  'delay',\n",
              "  'news',\n",
              "  'feed',\n",
              "  'delivery',\n",
              "  'moscow',\n",
              "  'international',\n",
              "  'aviation',\n",
              "  'school',\n",
              "  'aviation',\n",
              "  'school',\n",
              "  'poljot',\n",
              "  'meaning',\n",
              "  'flight',\n",
              "  'organized',\n",
              "  'moscow',\n",
              "  'aviation',\n",
              "  'institute',\n",
              "  'prominent',\n",
              "  'russian',\n",
              "  'center',\n",
              "  'airspace',\n",
              "  'education',\n",
              "  'foreign',\n",
              "  'trade',\n",
              "  'firm',\n",
              "  'poljot',\n",
              "  'well',\n",
              "  'known',\n",
              "  'various',\n",
              "  'parts',\n",
              "  'world',\n",
              "  'quartz',\n",
              "  'mechanical',\n",
              "  'wrist',\n",
              "  'watches',\n",
              "  'course',\n",
              "  'studies',\n",
              "  'last',\n",
              "  'days',\n",
              "  'time',\n",
              "  'unique',\n",
              "  'opportunity',\n",
              "  'listen',\n",
              "  'intensive',\n",
              "  'courses',\n",
              "  'main',\n",
              "  'aviation',\n",
              "  'disciplines',\n",
              "  'history',\n",
              "  'theory',\n",
              "  'techniques',\n",
              "  'design',\n",
              "  'airplanes',\n",
              "  'visit',\n",
              "  'get',\n",
              "  'acquainted',\n",
              "  'world',\n",
              "  'known',\n",
              "  'russian',\n",
              "  'aviation',\n",
              "  'firms',\n",
              "  'tu',\n",
              "  'mig',\n",
              "  'yak',\n",
              "  'il',\n",
              "  'su',\n",
              "  'meet',\n",
              "  'discussions',\n",
              "  'famous',\n",
              "  'aviation',\n",
              "  'scientists',\n",
              "  'engineers',\n",
              "  'pilots',\n",
              "  'visit',\n",
              "  'interesting',\n",
              "  'museums',\n",
              "  'unique',\n",
              "  'aviation',\n",
              "  'techniques',\n",
              "  'closed',\n",
              "  'many',\n",
              "  'years',\n",
              "  'public',\n",
              "  'see',\n",
              "  'international',\n",
              "  'airspace',\n",
              "  'show',\n",
              "  'take',\n",
              "  'place',\n",
              "  'moscow',\n",
              "  'august',\n",
              "  'september',\n",
              "  'visit',\n",
              "  'famous',\n",
              "  'art',\n",
              "  'museums',\n",
              "  'historical',\n",
              "  'architectural',\n",
              "  'monuments',\n",
              "  'theatres',\n",
              "  'concert',\n",
              "  'halls',\n",
              "  'take',\n",
              "  'part',\n",
              "  'sport',\n",
              "  'competitions',\n",
              "  'great',\n",
              "  'time',\n",
              "  'new',\n",
              "  'friends',\n",
              "  'director',\n",
              "  'school',\n",
              "  'mr',\n",
              "  'oleg',\n",
              "  'samelovich',\n",
              "  'well',\n",
              "  'known',\n",
              "  'russian',\n",
              "  'scientist',\n",
              "  'professor',\n",
              "  'general',\n",
              "  'designer',\n",
              "  'chief',\n",
              "  'airplanes',\n",
              "  'design',\n",
              "  'department',\n",
              "  'moscow',\n",
              "  'aviation',\n",
              "  'institute',\n",
              "  'mr',\n",
              "  'samelovich',\n",
              "  'one',\n",
              "  'designers',\n",
              "  'su',\n",
              "  'su',\n",
              "  'su',\n",
              "  'lectures',\n",
              "  'given',\n",
              "  'english',\n",
              "  'using',\n",
              "  'multi',\n",
              "  'media',\n",
              "  'concept',\n",
              "  'students',\n",
              "  'provided',\n",
              "  'necessary',\n",
              "  'text',\n",
              "  'books',\n",
              "  'literature',\n",
              "  'full',\n",
              "  'course',\n",
              "  'studies',\n",
              "  'completed',\n",
              "  'student',\n",
              "  'receive',\n",
              "  'special',\n",
              "  'certificate',\n",
              "  'graduation',\n",
              "  'cost',\n",
              "  'studies',\n",
              "  'including',\n",
              "  'hotel',\n",
              "  'meals',\n",
              "  'excursions',\n",
              "  'theatres',\n",
              "  'etc',\n",
              "  'apply',\n",
              "  'admission',\n",
              "  'send',\n",
              "  'application',\n",
              "  'moscow',\n",
              "  'marksistskaja',\n",
              "  'foreign',\n",
              "  'trade',\n",
              "  'firm',\n",
              "  'poljot',\n",
              "  'phone',\n",
              "  'fax',\n",
              "  'polex',\n",
              "  'su',\n",
              "  'telex',\n",
              "  'application',\n",
              "  'include',\n",
              "  'full',\n",
              "  'name',\n",
              "  'address',\n",
              "  'date',\n",
              "  'place',\n",
              "  'birth',\n",
              "  'addition',\n",
              "  'include',\n",
              "  'complete',\n",
              "  'passport',\n",
              "  'information',\n",
              "  'well',\n",
              "  'description',\n",
              "  'education',\n",
              "  'upon',\n",
              "  'receipt',\n",
              "  'information',\n",
              "  'poljot',\n",
              "  'immediately',\n",
              "  'forward',\n",
              "  'official',\n",
              "  'invitation',\n",
              "  'obtaining',\n",
              "  'russian',\n",
              "  'entrance',\n",
              "  'visa',\n",
              "  'well',\n",
              "  'details',\n",
              "  'payment',\n",
              "  'require',\n",
              "  'additional',\n",
              "  'information',\n",
              "  'please',\n",
              "  'hesitate',\n",
              "  'contact',\n",
              "  'us',\n",
              "  'signed',\n",
              "  'samelovich',\n",
              "  'steve',\n",
              "  'emmett',\n",
              "  'semmett',\n",
              "  'gmuvax',\n",
              "  'gmu',\n",
              "  'edu',\n",
              "  'csi',\n",
              "  'physics',\n",
              "  'george',\n",
              "  'mason',\n",
              "  'university'],\n",
              " ['avery',\n",
              "  'gestalt',\n",
              "  'stanford',\n",
              "  'edu',\n",
              "  'avery',\n",
              "  'wang',\n",
              "  'subject',\n",
              "  'serial',\n",
              "  'line',\n",
              "  'connection',\n",
              "  'duo',\n",
              "  'pc',\n",
              "  'organization',\n",
              "  'dso',\n",
              "  'stanford',\n",
              "  'university',\n",
              "  'lines',\n",
              "  'article',\n",
              "  'almaden',\n",
              "  'ibm',\n",
              "  'com',\n",
              "  'petrack',\n",
              "  'vnet',\n",
              "  'ibm',\n",
              "  'com',\n",
              "  'writes',\n",
              "  'tried',\n",
              "  'almost',\n",
              "  'everything',\n",
              "  'sun',\n",
              "  'get',\n",
              "  'null',\n",
              "  'modem',\n",
              "  'connection',\n",
              "  'mac',\n",
              "  'duo',\n",
              "  'pc',\n",
              "  'used',\n",
              "  'mackermit',\n",
              "  'versaterm',\n",
              "  'mac',\n",
              "  'side',\n",
              "  'used',\n",
              "  'procomm',\n",
              "  'kermit',\n",
              "  'softerm',\n",
              "  'os',\n",
              "  'pc',\n",
              "  'ps',\n",
              "  'side',\n",
              "  'used',\n",
              "  'non',\n",
              "  'hardware',\n",
              "  'handshaking',\n",
              "  'hardware',\n",
              "  'ahdshaking',\n",
              "  'cables',\n",
              "  'know',\n",
              "  'hands',\n",
              "  'shaking',\n",
              "  'effort',\n",
              "  'nothing',\n",
              "  'allowed',\n",
              "  'file',\n",
              "  'transfers',\n",
              "  'mac',\n",
              "  'ps',\n",
              "  'could',\n",
              "  'hear',\n",
              "  'someone',\n",
              "  'attesting',\n",
              "  'really',\n",
              "  'pump',\n",
              "  'information',\n",
              "  'serial',\n",
              "  'port',\n",
              "  'duo',\n",
              "  'fast',\n",
              "  'like',\n",
              "  'via',\n",
              "  'modem',\n",
              "  'via',\n",
              "  'sys',\n",
              "  'ex',\n",
              "  'dump',\n",
              "  'could',\n",
              "  'anyone',\n",
              "  'duo',\n",
              "  'help',\n",
              "  'going',\n",
              "  'absolutely',\n",
              "  'insane',\n",
              "  'wanna',\n",
              "  'know',\n",
              "  'problem',\n",
              "  'duo',\n",
              "  'duo',\n",
              "  'duos',\n",
              "  'hmmm',\n",
              "  'sounds',\n",
              "  'vaguely',\n",
              "  'similar',\n",
              "  'problem',\n",
              "  'long',\n",
              "  'time',\n",
              "  'ago',\n",
              "  'trying',\n",
              "  'use',\n",
              "  'kermit',\n",
              "  'building',\n",
              "  'serial',\n",
              "  'connection',\n",
              "  'duo',\n",
              "  'next',\n",
              "  'think',\n",
              "  'problem',\n",
              "  'handshaking',\n",
              "  'basically',\n",
              "  'need',\n",
              "  'make',\n",
              "  'sure',\n",
              "  'handshaking',\n",
              "  'protocol',\n",
              "  'sides',\n",
              "  'safe',\n",
              "  'place',\n",
              "  'start',\n",
              "  'selecting',\n",
              "  'handshaking',\n",
              "  'either',\n",
              "  'end',\n",
              "  'one',\n",
              "  'problem',\n",
              "  'zilog',\n",
              "  'serial',\n",
              "  'chip',\n",
              "  'seems',\n",
              "  'get',\n",
              "  'permanently',\n",
              "  'wedged',\n",
              "  'talk',\n",
              "  'wrong',\n",
              "  'reset',\n",
              "  'clear',\n",
              "  'know',\n",
              "  'specifics',\n",
              "  'could',\n",
              "  'nonlinearity',\n",
              "  'screws',\n",
              "  'attempts',\n",
              "  'debugging',\n",
              "  'system',\n",
              "  'could',\n",
              "  'well',\n",
              "  'things',\n",
              "  'right',\n",
              "  'eventually',\n",
              "  'one',\n",
              "  'wrong',\n",
              "  'move',\n",
              "  'like',\n",
              "  'trying',\n",
              "  'bad',\n",
              "  'handshaking',\n",
              "  'protocol',\n",
              "  'screw',\n",
              "  'correct',\n",
              "  'actions',\n",
              "  'next',\n",
              "  'machine',\n",
              "  'reset',\n",
              "  'wedged',\n",
              "  'mac',\n",
              "  'also',\n",
              "  'next',\n",
              "  'way',\n",
              "  'send',\n",
              "  'files',\n",
              "  'back',\n",
              "  'forth',\n",
              "  'duo',\n",
              "  'next',\n",
              "  'without',\n",
              "  'problem',\n",
              "  'pretty',\n",
              "  'high',\n",
              "  'speeds',\n",
              "  'know',\n",
              "  'kind',\n",
              "  'chip',\n",
              "  'pc',\n",
              "  'uses',\n",
              "  'think',\n",
              "  'zilog',\n",
              "  'pretty',\n",
              "  'standard',\n",
              "  'hope',\n",
              "  'helps',\n",
              "  'avery'],\n",
              " ['nicho',\n",
              "  'vnet',\n",
              "  'ibm',\n",
              "  'com',\n",
              "  'greg',\n",
              "  'stewart',\n",
              "  'nicholls',\n",
              "  'subject',\n",
              "  'biosphere',\n",
              "  'ii',\n",
              "  'reply',\n",
              "  'nicho',\n",
              "  'vnet',\n",
              "  'ibm',\n",
              "  'com',\n",
              "  'disclaimer',\n",
              "  'posting',\n",
              "  'represents',\n",
              "  'poster',\n",
              "  'views',\n",
              "  'ibm',\n",
              "  'news',\n",
              "  'software',\n",
              "  'ureply',\n",
              "  'x',\n",
              "  'x',\n",
              "  'nicho',\n",
              "  'vnet',\n",
              "  'ibm',\n",
              "  'com',\n",
              "  'q',\n",
              "  'ku',\n",
              "  'av',\n",
              "  'access',\n",
              "  'digex',\n",
              "  'net',\n",
              "  'lines',\n",
              "  'q',\n",
              "  'ku',\n",
              "  'av',\n",
              "  'access',\n",
              "  'digex',\n",
              "  'net',\n",
              "  'pat',\n",
              "  'writes',\n",
              "  'work',\n",
              "  'privately',\n",
              "  'funded',\n",
              "  'data',\n",
              "  'belongs',\n",
              "  'sbv',\n",
              "  'see',\n",
              "  'either',\n",
              "  'george',\n",
              "  'fred',\n",
              "  'scoriating',\n",
              "  'ibm',\n",
              "  'research',\n",
              "  'division',\n",
              "  'releasing',\n",
              "  'data',\n",
              "  'publish',\n",
              "  'plenty',\n",
              "  'kiddo',\n",
              "  'look',\n",
              "  'sig',\n",
              "  'files',\n",
              "  'like',\n",
              "  'strings',\n",
              "  'every',\n",
              "  'yo',\n",
              "  'yo',\n",
              "  'got',\n",
              "  'one',\n",
              "  'greg',\n",
              "  'nicholls',\n",
              "  'nicho',\n",
              "  'vnet',\n",
              "  'ibm',\n",
              "  'com',\n",
              "  'business',\n",
              "  'nicho',\n",
              "  'olympus',\n",
              "  'demon',\n",
              "  'co',\n",
              "  'uk',\n",
              "  'private'],\n",
              " ['michael',\n",
              "  'robert',\n",
              "  'peck',\n",
              "  'mp',\n",
              "  'j',\n",
              "  'andrew',\n",
              "  'cmu',\n",
              "  'edu',\n",
              "  'subject',\n",
              "  'x',\n",
              "  'video',\n",
              "  'iici',\n",
              "  'organization',\n",
              "  'freshman',\n",
              "  'electrical',\n",
              "  'computer',\n",
              "  'engineering',\n",
              "  'carnegie',\n",
              "  'mellon',\n",
              "  'pittsburgh',\n",
              "  'pa',\n",
              "  'lines',\n",
              "  'nntp',\n",
              "  'posting',\n",
              "  'host',\n",
              "  'po',\n",
              "  'andrew',\n",
              "  'cmu',\n",
              "  'edu',\n",
              "  'need',\n",
              "  'able',\n",
              "  'run',\n",
              "  'nec',\n",
              "  'fgx',\n",
              "  'x',\n",
              "  'mode',\n",
              "  'iici',\n",
              "  'done',\n",
              "  'right',\n",
              "  'video',\n",
              "  'card',\n",
              "  'video',\n",
              "  'card',\n",
              "  'michael']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77wJyzoCA_iK",
        "outputId": "e8af0ee3-57a0-4905-a7dc-37cebf91960c"
      },
      "source": [
        "%time model_data = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.72 s, sys: 48.6 ms, total: 4.77 s\n",
            "Wall time: 3.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMslTXwhA_ke"
      },
      "source": [
        "def sentiment(v, c):\n",
        "    model = Pipeline(\n",
        "        [(\"vectorizer\", v), \n",
        "         (\"classifier\", c)])\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print_accuracy_score_for_classes(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrI4tuBdA_mQ"
      },
      "source": [
        "class EmbeddingVectorizer(object):\n",
        "    '''\n",
        "    Для текста усредним вектора входящих в него слов\n",
        "    '''\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.size = model.vector_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([np.mean(\n",
        "            [self.model[w] for w in words if w in self.model] \n",
        "            or [np.zeros(self.size)], axis=0)\n",
        "            for words in X])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjzxFF20A_oW"
      },
      "source": [
        "# Обучающая и тестовая выборки\n",
        "boundary = 700\n",
        "X_train = corpus[:boundary] \n",
        "X_test = corpus[boundary:]\n",
        "y_train = newsgroups['target'][:boundary]\n",
        "y_test = newsgroups['target'][boundary:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiU-cDGSBTfK",
        "outputId": "7c31b18e-f493-42ec-d11f-fbd3b4778ea4"
      },
      "source": [
        "sentiment(EmbeddingVectorizer(model_data.wv), LogisticRegression(C=5.0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Метка \t Accuracy\n",
            "0 \t 0.9418604651162791\n",
            "1 \t 0.9347258485639687\n",
            "2 \t 0.8731343283582089\n",
            "3 \t 0.9086538461538461\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}